{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Data Lake with spark.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "wB0GCBdfVgPA"
      },
      "source": [
        "# Data Lake using AWS EMR Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "FMiXmjnJVgPK"
      },
      "source": [
        "## Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "HiD332_QVgPK"
      },
      "source": [
        "1. Install `awscli`\n",
        "\n",
        "2. run `aws configure` \n",
        "    * AWS Access Key ID : \n",
        "    * AWS Secret Access Key : \n",
        "    * Default region name: `us-west-2`\n",
        "    * Default output format : `json`\n",
        "    \n",
        "3. **copy all the necessary files to an s3 bucket**\n",
        "\n",
        "    * `emr_bootstrap.sh` &  `etl.py`\n",
        "    \n",
        "    `\n",
        "    #emr_bootstrap.sh file \n",
        "    #!/bin/bash\n",
        "    sudo easy_install pip3\n",
        "    `\n",
        "    \n",
        "    * Ex: `aws s3 cp <filename> s3://<bucket_name>`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "LJ6DhBeWVgPK"
      },
      "source": [
        "\n",
        "4. **Run EMR create script with the etl job**\n",
        "\n",
        "```\n",
        "aws emr create-cluster --name \"Spark cluster with step\" \\\n",
        "    --release-label emr-5.30.1 \\\n",
        "    --applications Name=Spark \\\n",
        "    --log-uri s3://dendsparktutorial/logs/ \\\n",
        "    --ec2-attributes KeyName=emr-key \\\n",
        "    --instance-type m5.xlarge \\\n",
        "    --instance-count 3 \\\n",
        "    --bootstrap-actions Path=s3://dendsparktutorial/emr_bootstrap.sh \\\n",
        "    --steps Type=Spark,Name=\"Spark program\",ActionOnFailure=CONTINUE,Args=[s3://dendsparktutorial/src/etl.py] \\\n",
        "    --use-default-roles \\\n",
        "    --auto-terminate\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "ARN-xOmpVgPK"
      },
      "source": [
        "**EMR Script Components**\n",
        "\n",
        "* **aws emr** : Invokes the AWS CLI, and specifically the command for EMR.\n",
        "\n",
        "* **create-cluster** : Creates a cluster\n",
        "* **--name** : You can give any name for this - this will show up on your AWS EMR UI. This can be duplicate as existing EMR.\n",
        "\n",
        "* **--release-label**: This is the version of EMR youâ€™d like to use.\n",
        "\n",
        "* **--instance-count**: Annotates instance count. One is for the primary, and the rest are for the secondary. For example, if --instance-count is given 4, then 1 instance will be reserved for primary, then 3 will be reserved for secondary instances.\n",
        "\n",
        "* **--applications**: List of applications you want to pre-install on your EMR at the launch time\n",
        "\n",
        "* **--bootstrap-actions**: You can have a script stored in S3 that pre-installs or sets\n",
        "environmental variables, and call that script at the time EMR launches\n",
        "\n",
        "* **--ec2-attributes KeyName**: Specify your permission key name, for example, if it is MyKey.pem, just specify MyKey for this field\n",
        "\n",
        "* **--instance-type**: Specify the type of instances you want to use. Detailed list can be accessed here, but find the one that can fit your data and your budget.\n",
        "\n",
        "* **--log-uri**: S3 location to store your EMR logs in. This log can store EMR metrics and also the metrics/logs for submission of your code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true,
        "id": "Fa0ddGCkVgPK",
        "outputId": "bb2f9305-39cc-470b-b571-7344200ba60d"
      },
      "source": [
        "!aws s3 ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-30 15:19:35 athena4dend\n",
            "2019-12-01 18:15:50 aws-athena-query-results-257082603396-us-west-1\n",
            "2019-11-02 16:18:57 aws-emr-resources-257082603396-us-west-2\n",
            "2019-11-02 16:17:06 aws-logs-257082603396-us-west-2\n",
            "2019-10-12 19:55:56 dendbucketdemo1\n",
            "2020-06-20 10:00:58 dendsparktut\n",
            "2020-06-29 15:37:02 dendsparktutorial\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true,
        "id": "fzdWzCksVgPK",
        "outputId": "1824b3d4-ff87-4b17-9d4f-c3db5cdac839"
      },
      "source": [
        "# I need to move the nesseccary files to an S3 bucket\n",
        "!ls *etl*\n",
        "!ls *emr*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "etl.py\tkoalas_etl.py\n",
            "emr_bootstrap.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true,
        "id": "F2lr79k-VgPK",
        "outputId": "15eb977c-197f-4f18-b7b9-eae1672a9a96"
      },
      "source": [
        "# copy your etl work + emr_bootstrap file if you are using koalas \n",
        "!aws s3 cp etl.py s3://dendsparktutorial/src\n",
        "!aws s3 cp emr_bootstrap.sh s3://dendsparktutorial"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "upload: ./etl.py to s3://dendsparktutorial/src                 \n",
            "upload: ./etl.py to s3://dendsparktutorial/src                 \n",
            "upload: ./emr_bootstrap.sh to s3://dendsparktutorial/emr_bootstrap.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true,
        "id": "XLtokRZFVgPK",
        "outputId": "61195675-1c3d-490d-b0cd-02f271abab86"
      },
      "source": [
        "# check \n",
        "!aws s3 ls dendsparktutorial/src/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-07-04 18:04:22       7793 etl.py\n",
            "2020-07-04 15:53:10       1226 koalas_etl.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "editable": true,
        "id": "C-ta4vFQVgPK",
        "outputId": "6c55429c-3696-4ddd-fd64-2e397f4b0c70"
      },
      "source": [
        "# run the spark job\n",
        "!aws emr create-cluster --name \"Spark cluster with step\" \\\n",
        "    --release-label emr-5.30.1 \\\n",
        "    --applications Name=Spark \\\n",
        "    --log-uri s3://dendsparktutorial/logs/ \\\n",
        "    --ec2-attributes KeyName=emr-key \\\n",
        "    --instance-type m5.xlarge \\\n",
        "    --instance-count 3 \\\n",
        "    --bootstrap-actions Path=s3://dendsparktutorial/emr_bootstrap.sh \\\n",
        "    --steps Type=Spark,Name=\"Spark program\",ActionOnFailure=CONTINUE,Args=[s3://dendsparktutorial/src/etl.py] \\\n",
        "    --use-default-roles \\\n",
        "    --auto-terminate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"ClusterId\": \"j-3CMP8BO03MU0N\"\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}